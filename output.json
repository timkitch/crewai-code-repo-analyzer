[{"source_filename": "/home/timkitch/ai-projects/crewai-code-repo-analyzer/remote_repo/confluence_rag_app/app.py", "programming_language": "python", "source_file_contents": "import streamlit as st\nfrom dotenv import load_dotenv\n\n# Import the ConfluenceQA class\nfrom confluence_qa import ConfluenceQA\n\n\nload_dotenv()\n\nst.set_page_config(\n    page_title='Q&A Bot for Confluence Page',\n    page_icon='\u26a1',\n    layout='wide',\n    initial_sidebar_state='auto',\n)\nif \"config\" not in st.session_state:\n    st.session_state[\"config\"] = {}\n\n# Initialize chat history if it doesn't exist\nif \"chat_history\" not in st.session_state:\n    st.session_state[\"chat_history\"] = \"\"\n\nif \"confluence_qa\" not in st.session_state:\n    confluence_qa = ConfluenceQA()\n    confluence_qa.init_embeddings()\n    confluence_qa.init_models()\n    confluence_qa.retrieval_qa_chain()\n    st.session_state[\"confluence_qa\"] = confluence_qa\n\n# @st.cache_resource\ndef load_confluence(config) -> None:\n    confluence_qa = st.session_state.get(\"confluence_qa\")\n    \n    confluence_qa.vector_db_confluence_docs(config)\n\nwith st.sidebar.form(key='cf-params'):\n    st.markdown('## Add your configs')\n    confluence_url = st.text_input(\"paste the confluence URL\", \"https://templates.atlassian.net/wiki/\")\n    username = st.text_input(label=\"confluence username\",\n                             help=\"leave blank if confluence page is public\"\n                             )\n    space_key = st.text_input(label=\"confluence space\",\n                              help=\"Space of Confluence\",\n                              value=\"RD\")\n    page_id = st.text_input(label=\"page id\",\n                            help=\"ID of Confluence page to ingest. Leave blank to ingest all pages in space\"\n                            )\n    api_key = st.text_input(label=\"confluence api key\",\n                            help=\"leave blank if confluence page is public\",\n                            type=\"password\")\n\n    submitted1 = st.form_submit_button(label='Submit')\n\n    st.session_state[\"config\"] = {\n        \"confluence_url\": confluence_url,\n        \"page_id\": page_id if page_id != \"\" else \"None\",\n        \"username\": username if username != \"\" else None,\n        \"api_key\": api_key if api_key != \"\" else None,\n        \"space_key\": space_key\n    }\n    \n    if submitted1:\n        with st.spinner(text=\"Ingesting Confluence...\"):\n            ### Hardcoding for https://templates.atlassian.net/wiki/ and space RD to avoid multiple OpenAI calls.\n            config = st.session_state[\"config\"]\n            # if config[\"confluence_url\"] == \"https://templates.atlassian.net/wiki/\" and config[\"space_key\"] == \"RD\":\n            #     config[\"persist_directory\"] = \".chroma_db\"\n            st.session_state[\"config\"] = config\n\n            load_confluence(st.session_state[\"config\"])\n        st.write(\"Confluence Space Ingested\")\n    # else:\n    #     st.write(\"Restored previously stored Confluence data\")\n\nst.title(\"Confluence Q&A Demo\")\n\nquestion = st.text_input('Ask a question', placeholder=\"What's the most common cause of ELK issues?\")\n\nif st.button('Get Answer', key='button2'):\n    with st.spinner(text=\"Asking LLM...\"):\n        confluence_qa = st.session_state.get(\"confluence_qa\")\n        if not question:\n            st.write(\"Please enter a question.\")\n        elif not confluence_qa:\n            st.write(\"Please load Confluence page first.\")\n        else:\n            sources = []\n            sources_str = \"\"\n            answer, sources = confluence_qa.answer_confluence(question)\n            for source in sources:\n                sources_str += f\"{source}, \\n\"\n                \n            sources_str = sources_str.rstrip(\" ,\\n\")\n            \n            st.session_state[\"chat_history\"] += f\"You: {question}\\nBot: {answer}\\n\\nSources:\\n{sources_str}\\n================================\\n\"\n                        \n            output = f\"{answer}\\n\\nSources:\\n{sources_str}\"\n                            \n            st.write(output)\n        \n            \n# Display chat history\nst.text_area(\"Chat History\", st.session_state[\"chat_history\"], height=600)"}, {"source_filename": "/home/timkitch/ai-projects/crewai-code-repo-analyzer/remote_repo/confluence_rag_app/utils.py", "programming_language": "python", "source_file_contents": "def pretty_print_docs(docs):\n    print(\n        f\"\\n{'-' * 100}\\n\".join(\n            [f\"Document {i+1}:\\n\\n\" + d.page_content for i, d in enumerate(docs)]\n        )\n    )"}, {"source_filename": "/home/timkitch/ai-projects/crewai-code-repo-analyzer/remote_repo/confluence_rag_app/rerank.py", "programming_language": "python", "source_file_contents": "from langchain_core.prompts import ChatPromptTemplate\nfrom langchain_openai import ChatOpenAI, OpenAIEmbeddings\nfrom langchain_community.vectorstores import Chroma\n\nfrom langchain.chains.combine_documents import create_stuff_documents_chain\nfrom langchain.chains import create_retrieval_chain\n\nfrom langchain.retrievers import ContextualCompressionRetriever\nfrom langchain.retrievers.document_compressors import FlashrankRerank\n\nfrom dotenv import load_dotenv\n\n# import pretty print\nfrom utils import pretty_print_docs\nfrom constants import *\n\nload_dotenv()\n\nstandard_retriever = None\nstandard_retrieval_chain = None\nreranked_retriever = None\nreranked_retrieval_chain = None\n\n\ndef setup():\n    # prompt = ChatPromptTemplate.from_template(\n    #             \"\"\"Answer the following question based only on the provided context. Do not consider any external information or any other context than that provided directly to you. Your answer should be based solely on the information in the context below. Do not use any knowledge you were trained on or any external information beyond what is directly presented. Here is the context:\n\n    #             <context>\n    #             {context}\n    #             </context>\n\n    #             Question: {input}\n\n    #             \"\"\",\n    #             role=\n    #             \"system\",\n    #         )\n\n    prompt = ChatPromptTemplate.from_template(\n        \"\"\"**Instructions for the Model:**\n\n        1. Answer the following question by strictly using only the context provided below.\n        2. Do not incorporate any external information, knowledge you were trained on, or assumptions not directly supported by the context.\n        3. Your response should derive solely from the given context. Ignore any prior knowledge or data not explicitly included in the context.\n        4. Avoid including any command line examples, technical jargon, or detailed explanations not explicitly mentioned or directly inferable from the context.\n        5. If the context does not fully answer the question, provide the best possible inference based solely on the available information, clearly stating any limitations due to lack of context.\n\n        **Context:**\n\n        <Context>\n        {context}\n        </Context>\n\n        **Question:**\n        {input}\n\n        **Response:**\n\n                \"\"\",\n        role=\"system\",\n    )\n\n    # prompt = ChatPromptTemplate.from_template(\n    #             \"\"\"You are now operating in a closed-book mode, meaning you should only use the information provided in the context below to generate your answer. Do not use any knowledge you were trained on or any external information beyond what is directly presented. Consider only the content within the explicitly defined context for constructing your response.\n\n    #             <context>\n    #             {context}\n    #             </context>\n\n    #             Based on the above context and only this context, answer the following question:\n\n    #             Question: {input}\n\n    #             Remember, disregard any pre-existing knowledge or information not found directly within the provided context.\n\n    #             \"\"\",\n    #             role=\"system\",\n    #         )\n\n    # OpenAI ada embeddings API\n    embedding = OpenAIEmbeddings()\n\n    vectordb = Chroma(\n        persist_directory=DB_DIRECTORY,\n        embedding_function=embedding,\n        collection_name=DB_COLLECTION_NAME,\n    )\n\n    llm = ChatOpenAI(model_name=LLM, temperature=0.0)\n    # llm = ChatOpenAI(model_name=\"gemma\", temperature=0.0, base_url=\"http://localhost:11434/v1\")\n\n    document_chain = create_stuff_documents_chain(llm=llm, prompt=prompt)\n\n    standard_retriever = vectordb.as_retriever(\n        search_type=\"similarity_score_threshold\",\n        search_kwargs={\"score_threshold\": 0.5, \"k\": 6},\n    )\n    standard_retrieval_chain = create_retrieval_chain(\n        standard_retriever, document_chain\n    )\n\n    compressor = FlashrankRerank()\n    reranked_retriever = ContextualCompressionRetriever(\n        base_compressor=compressor, base_retriever=standard_retriever\n    )\n\n    reranked_retrieval_chain = create_retrieval_chain(\n        reranked_retriever, document_chain\n    )\n\n    return (\n        standard_retriever,\n        standard_retrieval_chain,\n        reranked_retriever,\n        reranked_retrieval_chain,\n    )\n\n\ndef invoke_standard_retrieval_chain(query, verbose=False):\n    response = standard_retrieval_chain.invoke({\"input\": query})\n    if verbose:\n        print(\"\\n ****** original docs:\")\n        standard_docs = standard_retriever.get_relevant_documents(query)\n        pretty_print_docs(standard_docs)\n    return response\n\n\ndef invoke_reranked_retrieval_chain(query, verbose=False):\n    response = reranked_retrieval_chain.invoke({\"input\": query})\n    if verbose:\n        print(\"\\n ****** compressed docs:\")\n        compressed_docs = reranked_retriever.get_relevant_documents(query)\n        pretty_print_docs(compressed_docs)\n    return response\n\n\n(\n    standard_retriever,\n    standard_retrieval_chain,\n    reranked_retriever,\n    reranked_retrieval_chain,\n) = setup()\n\n\ndef main():\n    setup()\n    user_input = \"\"\n    while user_input != \"x\":\n        user_input = input(\"Enter your query or 'x' to exit: \")\n        if user_input != \"x\":\n            # standard_response = invoke_standard_retrieval_chain(user_input, verbose=True)\n            # print(f\"\\n ***** standard_response: {standard_response['answer']}\")\n            reranked_response = invoke_reranked_retrieval_chain(\n                user_input, verbose=True\n            )\n            print(f\"\\n ***** reranked_response: {reranked_response['answer']}\")\n\n\nif __name__ == \"__main__\":\n    main()\n"}, {"source_filename": "/home/timkitch/ai-projects/crewai-code-repo-analyzer/remote_repo/confluence_rag_app/constants.py", "programming_language": "python", "source_file_contents": "# Constants\nEMB_OPENAI_ADA = \"text-embedding-ada-002\"\nEMB_SBERT = None # Chroma takes care\n\nLLM = \"gpt-3.5-turbo-0125\"\n# LLM = \"gpt-3.5-turbo\"\n# LLM = \"gpt-4-turbo-preview\"\n\nDB_DIRECTORY = \".chroma_db\"\n\nDB_COLLECTION_NAME = \"confluence_qa\""}, {"source_filename": "/home/timkitch/ai-projects/crewai-code-repo-analyzer/remote_repo/confluence_rag_app/confluence_qa.py", "programming_language": "python", "source_file_contents": "from langchain_community.document_loaders import ConfluenceLoader\n\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain_openai import ChatOpenAI, OpenAIEmbeddings\n\nfrom langchain_community.vectorstores import Chroma\n\nfrom langchain.chains.combine_documents import create_stuff_documents_chain\nfrom langchain.chains import create_retrieval_chain\nfrom langchain_core.prompts import ChatPromptTemplate\n\nfrom langchain.retrievers import ContextualCompressionRetriever\nfrom langchain.retrievers.document_compressors import FlashrankRerank\n\nfrom typing import Tuple, List\n\nfrom constants import *\nfrom utils import pretty_print_docs\n\nclass ConfluenceQA:\n    def __init__(self):\n        self.embedding = None\n        self.vectordb = None\n        self.llm = None\n        self.qa = None\n        self.retriever = None\n        self.retrieval_chain = None\n\n        self.prompt = ChatPromptTemplate.from_template(\n            \"\"\"Answer the following question based only on the provided context:\n                                                     \n            <context>\n            {context}\n            </context>\n            \n            Question: {input}\n            \n            \"\"\",\n            role=\"system\",\n        )\n        \n        self.retriever = None\n        self.retrieval_chain = None\n\n    def init_embeddings(self) -> None:\n        # OpenAI ada embeddings API\n        self.embedding = OpenAIEmbeddings()\n\n        self.vectordb = Chroma(\n            persist_directory=DB_DIRECTORY,\n            embedding_function=self.embedding,\n            collection_name=DB_COLLECTION_NAME,\n        )\n\n    def init_models(self) -> None:\n        # OpenAI GPT\n        self.llm = ChatOpenAI(model_name=LLM, temperature=0.0)\n        \n        # self.llm = ChatOpenAI(model_name=\"gemma\", temperature=0.0, base_url=\"http://localhost:11434/v1\")\n\n        # Use local LLM hosted by LM Studio\n        # self.llm = ChatOpenAI(\n        #     openai_api_key = \"NULL\",\n        #     temperature = 0,\n        #     openai_api_base = \"http://localhost:1234/v1\"\n        # )\n\n    # TODO implement purge_data\n    # def purge_data(self) -> None:\n    #     \"\"\"\n    #     Clears the DB.\n    #     \"\"\"\n    #     print(\"Clearing the DB. Collections before purge...\")\n    #     self.persistent_client.list_collections()\n    #     self.persistent_client.delete_collection(DB_COLLECTION_NAME)\n    #     print(\"Collections after purge...\")\n    #     self.persistent_client.list_collections()\n    #     print(\"Cleared the DB.\")\n\n    def vector_db_confluence_docs(self, config: dict = {}) -> None:\n        \"\"\"\n        Extracts documents from Confluence, splits them into chunks, and adds them to the database.\n\n        Args:\n            config (dict): A dictionary containing the configuration parameters.\n                - confluence_url (str): The URL of the Confluence instance.\n                - username (str): The username for authentication.\n                - api_key (str): The API key for authentication.\n                - space_key (str): The key of the Confluence space.\n                - page_id (str): The ID of the Confluence page. If None, all pages in the space will be loaded.\n\n        Returns:\n            None\n        \"\"\"\n        confluence_url = config.get(\"confluence_url\", None)\n        username = config.get(\"username\", None)\n        api_key = config.get(\"api_key\", None)\n        space_key = config.get(\"space_key\", None)\n        page_id = config.get(\"page_id\", None)\n\n        ## 1. Extract the documents\n        loader = ConfluenceLoader(\n            url=confluence_url, username=username, api_key=api_key\n        )\n\n        if page_id and page_id != \"None\":\n            documents = loader.load(\n                space_key=space_key, page_ids=[page_id], max_pages=1\n            )\n        else:\n            documents = loader.load(space_key=space_key, limit=100, max_pages=1000)\n\n        ## 2. Split the documents\n        text_splitter = RecursiveCharacterTextSplitter(\n            chunk_size=2000, chunk_overlap=300\n        )\n        texts = text_splitter.split_documents(documents)\n\n        ## 3. Add the documents to the DB\n        if self.vectordb:\n            print(\"count before\", self.vectordb._collection.count())\n            self.vectordb.add_documents(documents=texts)\n            self.vectordb.persist()\n            print(\"count after\", self.vectordb._collection.count())\n        else:\n            # this should never happen?\n            print(\"DB not initialized. Creating new DB from docs...\")\n            self.vectordb = Chroma.from_documents(\n                documents=texts,\n                embedding=self.embedding,\n                persist_directory=DB_DIRECTORY,\n            )\n            self.persistent_client = self.vectordb.PersistentClient()\n            print(\"count after\", self.vectordb._collection.count())\n\n    def retrieval_qa_chain(self) -> None:\n        \"\"\"\n        Retrieves a question-answer chain using a custom prompt.\n\n        Returns:\n            None\n        \"\"\"\n        document_chain = create_stuff_documents_chain(llm=self.llm, prompt=self.prompt)\n\n        # TODO make threshold configurable\n        self.retriever = self.vectordb.as_retriever(\n            search_type=\"similarity_score_threshold\",\n            search_kwargs={\"score_threshold\": 0.7, \"k\": 5},\n        )\n                \n        compressor = FlashrankRerank()\n        reranked_retriever = ContextualCompressionRetriever(\n            base_compressor=compressor, base_retriever=self.retriever\n            )\n    \n        self.retrieval_chain = create_retrieval_chain(reranked_retriever, document_chain)\n\n    def answer_confluence(self, question: str) -> Tuple[str, List[str]]:\n        # Your code here\n        \"\"\"\n        Answers a question using the Confluence QA system.\n\n        Args:\n            question (str): The question to be answered.\n\n        Returns:\n            str: The answer to the question.\n        \"\"\"\n        response = self.retrieval_chain.invoke({\"input\": question})\n\n        answer = response[\"answer\"]\n\n        # we don't want duplicates in sources\n        sources = set()\n        \n        compressed_docs = self.retriever.get_relevant_documents(question)\n        for doc in compressed_docs:\n            sources.add(doc.metadata[\"source\"])\n        \n        # if using FlashrankRerank, the doc metata is not available in the response\n        # for doc in response[\"context\"]:\n        #     sources.add(doc.metadata[\"source\"])\n\n        print(f\"Number of sources: {len(sources)}\")\n        for source in sources:\n            print(f\"Sources: {source}\")\n\n        if not answer:\n            print(\"LLM could not provide any answer.\")\n            answer = \"Sorry, it seems I lack the domain information to answer that question. Try adding the data and ask again.\"\n\n        return answer, sources\n"}]